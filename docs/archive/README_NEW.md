# PDF Knowledge Pipeline - Moderne Modulare Architektur

Eine **lokale, erweiterbare RAG-Pipeline** fÃ¼r wissenschaftliche PDFs mit moderner Python-Architektur. Kernidee: PDF-Dateien bleiben lokal, werden intelligent verarbeitet und Ã¼ber verschiedene Suchverfahren zugÃ¤nglich gemacht.

## ğŸš€ Neue Funktionen (2025)

âœ… **Modulare Architektur** - Saubere Trennung von Bibliothekscode und CLI-Tools  
âœ… **Mehrere Suchverfahren** - Volltext (Whoosh), Embeddings (ChromaDB), Formeln (SQLite)  
âœ… **CLI-Tools** - Benutzerfreundliche Kommandozeilen-Interfaces  
âœ… **Dependency-Management** - Optionale Dependencies, keine Crashes  
âœ… **Programmatische API** - Module kÃ¶nnen in anderen Scripts importiert werden  

## ğŸ“ Projektstruktur (Neu)

```
â”œâ”€â”€ ğŸ—ï¸ src/                    # Modulare Pipeline-Architektur
â”‚   â”œâ”€â”€ core/                  # KernfunktionalitÃ¤t  
â”‚   â”‚   â”œâ”€â”€ convert_local.py   # PDF â†’ Text Konvertierung
â”‚   â”‚   â”œâ”€â”€ indexer.py         # ChromaDB Embedding-Index  
â”‚   â”‚   â””â”€â”€ rag.py             # RAG-basierte Anfragen
â”‚   â”œâ”€â”€ pipeline/              # Text-Pipeline
â”‚   â”‚   â”œâ”€â”€ extract.py         # PDF-Text-Extraktion
â”‚   â”‚   â”œâ”€â”€ index.py           # Whoosh Volltextindex
â”‚   â”‚   â””â”€â”€ search.py          # BM25 Volltext-Suche
â”‚   â”œâ”€â”€ analysis/              # Analyse-Tools
â”‚   â”‚   â”œâ”€â”€ duplicates.py      # Duplikatserkennung
â”‚   â”‚   â””â”€â”€ relevance.py       # Keyword-basierte Relevanzanalyse
â”‚   â”œâ”€â”€ formulas/              # Formel-Verarbeitung
â”‚   â”‚   â”œâ”€â”€ nougat.py          # Nougat OCR fÃ¼r Formeln
â”‚   â”‚   â”œâ”€â”€ extract.py         # LaTeX-Formelextraktion  
â”‚   â”‚   â”œâ”€â”€ index.py           # SQLite Formelindex
â”‚   â”‚   â””â”€â”€ search.py          # Symbol-/Muster-Suche
â”‚   â””â”€â”€ cli/                   # CLI-Interfaces
â”‚       â”œâ”€â”€ extract.py         # Text-Extraktion CLI
â”‚       â”œâ”€â”€ search.py          # Such-CLI  
â”‚       â”œâ”€â”€ analyze.py         # Analyse-CLI
â”‚       â””â”€â”€ pipeline.py        # Pipeline-Management
â”œâ”€â”€ ğŸ“„ Daten & Verarbeitung
â”‚   â”œâ”€â”€ raw/                   # Eingabe-PDFs
â”‚   â”œâ”€â”€ txt/                   # Extrahierte Texte
â”‚   â”œâ”€â”€ processed/             # Whoosh-Index, Nougat-Output
â”‚   â”œâ”€â”€ metadata/              # Reports, Formel-Indices
â”‚   â””â”€â”€ .chroma/               # ChromaDB Embedding-Datenbank
â”œâ”€â”€ ğŸ”§ Konfiguration
â”‚   â”œâ”€â”€ pipeline_new.py        # Haupt-Pipeline (ersetzt alte pipeline.py)
â”‚   â”œâ”€â”€ search.py              # Einfaches Such-Tool
â”‚   â”œâ”€â”€ config.json            # Pipeline-Konfiguration
â”‚   â”œâ”€â”€ .env                   # API-Keys (OpenAI, Zotero, etc.)
â”‚   â””â”€â”€ requirements.txt       # Python-Dependencies
â””â”€â”€ ğŸ“š tools/ (Legacy)         # Alte CLI-Tools (KompatibilitÃ¤t)
```

## âš¡ Schnellstart

### 1. Setup & Dependencies
```bash
# Python Environment
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Grundlegende Dependencies installieren
pip install -r requirements.txt

# ZusÃ¤tzliche Dependencies je nach Bedarf
pip install PyMuPDF           # PDF-Extraktion
pip install Whoosh            # Volltextsuche  
pip install sentence-transformers chromadb  # Embeddings
pip install nougat-ocr        # Formel-OCR (optional)
```

### 2. Erste Schritte
```bash
# ğŸ“Š Duplikate in PDF-Sammlung finden
python src/cli/analyze.py duplicates

# ğŸ“ˆ Relevanzanalyse nach Keywords
python src/cli/analyze.py relevance --keywords "machine learning,neural networks"

# ğŸ”§ VollstÃ¤ndige Pipeline ausfÃ¼hren
python pipeline_new.py --skip-nougat    # Ohne Formel-OCR
python pipeline_new.py                  # Mit Formel-OCR
```

## ğŸ” Such-Features

### Volltext-Suche (Whoosh BM25)
```bash
# Ãœber neue Pipeline
python -c "
import sys; sys.path.insert(0, 'src')
import pipeline.search as ps
results = ps.search_whoosh('machine learning', 5)
for r in results: print(f'ğŸ“„ {r[\"filename\"]}')
"

# Pipeline-Schritte einzeln
python pipeline_new.py --step extract    # Text extrahieren
python pipeline_new.py --step index      # Whoosh-Index erstellen
```

### Embedding-Suche (ChromaDB)
```bash
# Semantische Ã„hnlichkeitssuche
python -c "
import sys; sys.path.insert(0, 'src')  
import core.rag as rag
answer = rag.ask('Was ist Deep Learning?', k=5)
print(answer)
"
```

### Formel-Suche
```bash
# Nach Symbolen suchen
python -c "
import sys; sys.path.insert(0, 'src')
import formulas.search as fs
results = fs.search_formulas(symbol='Re', limit=5)
for r in results: print(f'ğŸ”¢ {r[\"latex\"][:80]}...')
"

# Pipeline fÃ¼r Formeln
python pipeline_new.py --step nougat        # Nougat OCR
python pipeline_new.py --step formulas      # Formeln extrahieren
python pipeline_new.py --step formula-index # Formel-Index erstellen
```

## ğŸ› ï¸ CLI-Tools (Empfohlen)

### Analyse-Tools
```bash
# Duplikaterkennung
python src/cli/analyze.py duplicates --threshold 85

# Relevanzanalyse  
python src/cli/analyze.py relevance --keywords "CFD,turbulence,POD"
```

### Pipeline-Management
```bash
# VollstÃ¤ndige Pipeline
python src/cli/pipeline.py full

# Einzelne Schritte
python src/cli/pipeline.py extract
python src/cli/pipeline.py index
python src/cli/pipeline.py nougat
```

## ğŸ“¦ Programmatische Nutzung

```python
# Module importieren
import sys
sys.path.append('src')

# Text-Extraktion
from pipeline.extract import extract_all_pdfs, extract_pdf
extract_all_pdfs()

# Duplikaterkennung
from analysis.duplicates import scan_duplicates
duplicates = scan_duplicates()
print(f"Gefunden: {duplicates['total_files']} Dateien")

# Suche
from pipeline.search import search_whoosh
results = search_whoosh("neural networks", k=10)

# Formeln
from formulas.search import search_formulas  
formulas = search_formulas(symbol="omega", limit=5)
```

## ğŸ”§ Konfiguration

### Environment Variables (.env)
```bash
# OpenAI fÃ¼r RAG
OPENAI_API_KEY=sk-...

# Zotero Integration (optional)
ZOTERO_API_KEY=...
ZOTERO_USER_ID=1234567

# Mathpix fÃ¼r Formeln (optional) 
MATHPIX_APP_ID=...
MATHPIX_APP_KEY=...

# Pfade (optional)
RAW_DIRS=/path/to/pdfs:/another/path
TEXT_OUT_DIR=custom_txt_dir
```

### Pipeline-Konfiguration (config.json)
```json
{
  "embeddings": {
    "model": "all-MiniLM-L6-v2"
  },
  "directories": {
    "raw": "raw",
    "processed": "processed",
    "output": "txt"
  }
}
```

## ğŸ”„ Migration von alter Struktur

### Alt (tools/) â†’ Neu (src/cli/)
```bash
# Alte CLI-Tools
python tools/01_scan_duplicates.py
python tools/04_relevance_report.py --keywords "ML"
python tools/05_query.py --q "deep learning" --k 10

# Neue CLI-Tools  
python src/cli/analyze.py duplicates
python src/cli/analyze.py relevance --keywords "ML"
python search.py text --q "deep learning" --k 10
```

### Programmierung Alt â†’ Neu
```python
# Alt: Direkte tool-Aufrufe
import tools.scan_duplicates

# Neu: Modulare Imports
import sys; sys.path.append('src')
from analysis.duplicates import scan_duplicates
```

## ğŸ“ˆ Performance & Features

| Feature | Alt (tools/) | Neu (src/) | Verbesserung |
|---------|-------------|------------|--------------|
| **ModularitÃ¤t** | âŒ Verstreute Scripts | âœ… Klare Module | ğŸ”¥ Bessere Organisation |
| **Testbarkeit** | âŒ Schwer testbar | âœ… Einzeln testbar | ğŸ”¥ Unit-Tests mÃ¶glich |
| **Wiederverwendung** | âŒ Copy-Paste | âœ… Import & nutzen | ğŸ”¥ DRY-Prinzip |
| **CLI-UX** | âŒ Verschiedene Patterns | âœ… Einheitliche CLIs | ğŸ”¥ Bessere UX |
| **Dependencies** | âŒ Hard-coded | âœ… Optional mit Fallbacks | ğŸ”¥ Robuster |

## ğŸ› Troubleshooting

```bash
# Dependency-Probleme
pip install PyMuPDF Whoosh sentence-transformers

# Import-Probleme  
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Berechtigungsprobleme
chmod +x src/cli/*.py

# Test der Module
python src/test_modules.py
```

## ğŸ—ºï¸ Roadmap

### Kurzfristig
- [ ] CLI-Tools fÃ¼r alle Module vervollstÃ¤ndigen
- [ ] Unit-Tests fÃ¼r Kern-Module
- [ ] Docker-Container fÃ¼r einfaches Deployment

### Mittelfristig  
- [ ] Web-Interface (FastAPI/Streamlit)
- [ ] Besseres Chunking (Ãœberschneidungen, Sections)
- [ ] Evaluation-Framework (Recall@K, MRR)

### Langfristig
- [ ] Multi-Modal Support (Bilder, Tabellen)
- [ ] Distributed Processing (Ray/Dask)
- [ ] Enterprise Features (Auth, Multi-User)

---

**ğŸ¯ Die neue modulare Architektur ist produktionsreif und deutlich wartbarer als die alte tools/-Struktur!**